{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = (2320, 50, 50, 3)\n",
      "y_train.shape = (2320,)\n",
      "X_validation.shape = (258, 50, 50, 3)\n",
      "y_validation.shape = (258,)\n"
     ]
    }
   ],
   "source": [
    "# load dataset and labels\n",
    "X_train = pickle.load(open('pickle_data/X_train.pickle','rb'))\n",
    "y_train = pickle.load(open('pickle_data/y_train.pickle','rb'))\n",
    "\n",
    "# load validation dataset and labels\n",
    "X_validation = pickle.load(open('pickle_data/X_validation.pickle', 'rb'))\n",
    "y_validation = pickle.load(open('pickle_data/y_validation.pickle', 'rb'))\n",
    "\n",
    "print('X_train.shape =', X_train.shape)\n",
    "print('y_train.shape =', y_train.shape)\n",
    "print('X_validation.shape =', X_validation.shape)\n",
    "print('y_validation.shape =', y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pixel data\n",
    "X_train = X_train/255.0\n",
    "X_validation = X_validation/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(conv_layer, layer_size, dense_layer):\n",
    "    # create a new log of the model to analyze results\n",
    "    tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1st convolution\n",
    "    model.add(Conv2D(layer_size, (3,3), input_shape=X_train.shape[1:]))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # add additional conv layers\n",
    "    for _ in range(conv_layer-1):\n",
    "        # convolution\n",
    "        model.add(Conv2D(layer_size, (3,3)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        # Max-Pool to reduce tensor size while retaining distict features\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    # flatten model for fully connected layers\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # add dense layers\n",
    "    for _ in range(dense_layer):\n",
    "        # fully-connected\n",
    "        model.add(Dense(layer_size))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    # final fully-connected layer (5 matches number of Pokemon categories)\n",
    "    model.add(Dense(5))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    model.fit(X_train, y_train, batch_size=16, epochs=20, validation_split=0.2, callbacks=[tensorboard])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1856 samples, validate on 464 samples\n",
      "Epoch 1/20\n",
      "1856/1856 [==============================] - 7s 4ms/sample - loss: 0.7879 - accuracy: 0.7015 - val_loss: 1.2941 - val_accuracy: 0.4009\n",
      "Epoch 2/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.4972 - accuracy: 0.8249 - val_loss: 0.8208 - val_accuracy: 0.7026\n",
      "Epoch 3/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.4737 - accuracy: 0.8362 - val_loss: 0.8335 - val_accuracy: 0.7284\n",
      "Epoch 4/20\n",
      "1856/1856 [==============================] - 5s 3ms/sample - loss: 0.3890 - accuracy: 0.8621 - val_loss: 0.6641 - val_accuracy: 0.7522\n",
      "Epoch 5/20\n",
      "1856/1856 [==============================] - 5s 3ms/sample - loss: 0.3068 - accuracy: 0.8906 - val_loss: 1.0007 - val_accuracy: 0.6983\n",
      "Epoch 6/20\n",
      "1856/1856 [==============================] - 5s 3ms/sample - loss: 0.2877 - accuracy: 0.9025 - val_loss: 0.7598 - val_accuracy: 0.7759\n",
      "Epoch 7/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.2753 - accuracy: 0.9089 - val_loss: 0.7616 - val_accuracy: 0.8017\n",
      "Epoch 8/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.2198 - accuracy: 0.9235 - val_loss: 0.4762 - val_accuracy: 0.8448\n",
      "Epoch 9/20\n",
      "1856/1856 [==============================] - 5s 3ms/sample - loss: 0.1966 - accuracy: 0.9397 - val_loss: 0.7029 - val_accuracy: 0.7672\n",
      "Epoch 10/20\n",
      "1856/1856 [==============================] - 5s 3ms/sample - loss: 0.1705 - accuracy: 0.9434 - val_loss: 0.4423 - val_accuracy: 0.8599\n",
      "Epoch 11/20\n",
      "1856/1856 [==============================] - 5s 3ms/sample - loss: 0.1601 - accuracy: 0.9472 - val_loss: 0.4928 - val_accuracy: 0.8556\n",
      "Epoch 12/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.1694 - accuracy: 0.9391 - val_loss: 0.9954 - val_accuracy: 0.7328\n",
      "Epoch 13/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.1151 - accuracy: 0.9623 - val_loss: 0.5613 - val_accuracy: 0.8233\n",
      "Epoch 14/20\n",
      "1856/1856 [==============================] - 7s 4ms/sample - loss: 0.1287 - accuracy: 0.9520 - val_loss: 0.4368 - val_accuracy: 0.8772\n",
      "Epoch 15/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.1038 - accuracy: 0.9650 - val_loss: 0.6395 - val_accuracy: 0.8341\n",
      "Epoch 16/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.1083 - accuracy: 0.9639 - val_loss: 0.5970 - val_accuracy: 0.8125\n",
      "Epoch 17/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.1024 - accuracy: 0.9655 - val_loss: 0.6690 - val_accuracy: 0.8190\n",
      "Epoch 18/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.0702 - accuracy: 0.9774 - val_loss: 0.4079 - val_accuracy: 0.8901\n",
      "Epoch 19/20\n",
      "1856/1856 [==============================] - 5s 3ms/sample - loss: 0.0820 - accuracy: 0.9698 - val_loss: 0.7860 - val_accuracy: 0.8082\n",
      "Epoch 20/20\n",
      "1856/1856 [==============================] - 6s 3ms/sample - loss: 0.0898 - accuracy: 0.9688 - val_loss: 0.8331 - val_accuracy: 0.7931\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "conv_layer = 4\n",
    "layer_size = 64\n",
    "dense_layer = 2\n",
    "\n",
    "# name of model to keep track of changing model architectures\n",
    "NAME = '{}-conv-{}-nodes-{}-dense-{}'.format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "    \n",
    "# create model and fit it\n",
    "model = create_model(conv_layer, layer_size, dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 86.05%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_validation, y_validation, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ericnunn/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/4-conv-128-nodes-1-dense-1575226904/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "model.save('models/' + NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
